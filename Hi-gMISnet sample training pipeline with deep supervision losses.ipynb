{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4012349,"sourceType":"datasetVersion","datasetId":2378526},{"sourceId":4424093,"sourceType":"datasetVersion","datasetId":2591346},{"sourceId":4981332,"sourceType":"datasetVersion","datasetId":2887789},{"sourceId":5576509,"sourceType":"datasetVersion","datasetId":3209844},{"sourceId":5676529,"sourceType":"datasetVersion","datasetId":3263240},{"sourceId":5785734,"sourceType":"datasetVersion","datasetId":3324210},{"sourceId":5810159,"sourceType":"datasetVersion","datasetId":3332587},{"sourceId":6278110,"sourceType":"datasetVersion","datasetId":3405163},{"sourceId":6412452,"sourceType":"datasetVersion","datasetId":3694983,"isSourceIdPinned":true},{"sourceId":6412997,"sourceType":"datasetVersion","datasetId":3414950},{"sourceId":6570033,"sourceType":"datasetVersion","datasetId":3793236}],"dockerImageVersionId":30302,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Hyperparameters (Learning rate, Scheduler, etc.) need to be optimized per dataset to get the best results. Use tensorflow version 2.8.4 and python version 3.7","metadata":{}},{"cell_type":"code","source":"from keras.utils.generic_utils import get_custom_objects\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy.random import randint\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.initializers import RandomNormal\nfrom keras.models import Model\nfrom keras import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import Conv2DTranspose\nfrom keras.layers import LeakyReLU\nfrom keras.layers import Activation\nfrom keras.layers import Concatenate\nfrom keras.layers import Dropout\nfrom keras.layers import BatchNormalization\nfrom matplotlib import pyplot as plt\nfrom tensorflow.keras.utils import plot_model\nimport numpy\nfrom PIL import Image, ImageOps\nimport os\nfrom keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, BatchNormalization, Activation, add\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Input, GlobalAveragePooling2D, Multiply\n\nfrom keras.models import Model, model_from_json\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers import ELU, LeakyReLU\nfrom keras.utils.vis_utils import plot_model\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras import backend as K\nimport random","metadata":{"papermill":{"duration":17.673742,"end_time":"2023-01-11T09:46:28.721252","exception":false,"start_time":"2023-01-11T09:46:11.04751","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-09-14T18:22:32.048299Z","iopub.execute_input":"2024-09-14T18:22:32.049625Z","iopub.status.idle":"2024-09-14T18:22:37.411644Z","shell.execute_reply.started":"2024-09-14T18:22:32.049511Z","shell.execute_reply":"2024-09-14T18:22:37.410755Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# DWT pooling and Convolution Block","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow_wavelets","metadata":{"execution":{"iopub.status.busy":"2024-09-14T18:22:37.413306Z","iopub.execute_input":"2024-09-14T18:22:37.413868Z","iopub.status.idle":"2024-09-14T18:23:06.244866Z","shell.execute_reply.started":"2024-09-14T18:22:37.413837Z","shell.execute_reply":"2024-09-14T18:23:06.243665Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting tensorflow_wavelets\n  Downloading tensorflow_wavelets-1.0.29-py3-none-any.whl (25 kB)\nRequirement already satisfied: tensorflow-probability in /opt/conda/lib/python3.7/site-packages (from tensorflow_wavelets) (0.14.1)\nRequirement already satisfied: PyWavelets in /opt/conda/lib/python3.7/site-packages (from tensorflow_wavelets) (1.3.0)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (from tensorflow_wavelets) (2.6.4)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from PyWavelets->tensorflow_wavelets) (1.21.6)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->tensorflow_wavelets) (1.43.0)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->tensorflow_wavelets) (1.12)\nCollecting numpy>=1.17.3\n  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting tensorboard<2.7,>=2.6.0\n  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->tensorflow_wavelets) (1.1.0)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow->tensorflow_wavelets) (1.12.1)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->tensorflow_wavelets) (5.0)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow->tensorflow_wavelets) (1.6.3)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->tensorflow_wavelets) (0.4.0)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow->tensorflow_wavelets) (0.15.0)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->tensorflow_wavelets) (3.3.0)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->tensorflow_wavelets) (0.2.0)\nRequirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->tensorflow_wavelets) (2.6.0)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->tensorflow_wavelets) (3.19.4)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow->tensorflow_wavelets) (0.37.1)\nCollecting h5py~=3.1.0\n  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->tensorflow_wavelets) (2.6.0)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->tensorflow_wavelets) (1.15.0)\nCollecting typing-extensions<3.11,>=3.7\n  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->tensorflow_wavelets) (1.1.2)\nRequirement already satisfied: cloudpickle>=1.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow-probability->tensorflow_wavelets) (2.1.0)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.7/site-packages (from tensorflow-probability->tensorflow_wavelets) (0.1.7)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from tensorflow-probability->tensorflow_wavelets) (5.1.1)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow->tensorflow_wavelets) (1.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (2.28.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (2.2.2)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (59.8.0)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (1.35.0)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (1.8.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (0.4.6)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (3.3.7)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (0.6.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (0.2.7)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (4.2.4)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (4.8)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (4.13.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (3.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow->tensorflow_wavelets) (3.2.0)\nInstalling collected packages: typing-extensions, numpy, h5py, tensorboard, tensorflow_wavelets\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.1.1\n    Uninstalling typing_extensions-4.1.1:\n      Successfully uninstalled typing_extensions-4.1.1\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.6\n    Uninstalling numpy-1.21.6:\n      Successfully uninstalled numpy-1.21.6\n  Attempting uninstall: h5py\n    Found existing installation: h5py 3.7.0\n    Uninstalling h5py-3.7.0:\n      Successfully uninstalled h5py-3.7.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.10.1\n    Uninstalling tensorboard-2.10.1:\n      Successfully uninstalled tensorboard-2.10.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ndask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\nxarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\nrich 12.6.0 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you have typing-extensions 3.10.0.2 which is incompatible.\npytorch-lightning 1.7.7 requires tensorboard>=2.9.1, but you have tensorboard 2.6.0 which is incompatible.\npytorch-lightning 1.7.7 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\npytools 2022.1.12 requires typing-extensions>=4.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.3 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\nnnabla 1.31.0 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\njaxlib 0.3.22+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\njax 0.3.23 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\nflax 0.6.1 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\nflake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.13.0 which is incompatible.\nfeaturetools 1.11.1 requires numpy>=1.21.0, but you have numpy 1.19.5 which is incompatible.\ndask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\ndask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\ncmdstanpy 1.0.7 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\nallennlp 2.10.1 requires h5py>=3.6.0, but you have h5py 3.1.0 which is incompatible.\nallennlp 2.10.1 requires numpy>=1.21.4, but you have numpy 1.19.5 which is incompatible.\naioitertools 0.11.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\naiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.27.93 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed h5py-3.1.0 numpy-1.19.5 tensorboard-2.6.0 tensorflow_wavelets-1.0.29 typing-extensions-3.10.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pywt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow_wavelets.utils.helpers import *\nfrom tensorflow.keras.applications import VGG16,DenseNet201\n# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # for tensor flow warning\n# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n\n\nclass DWT(layers.Layer):\n\n    def __init__(self, wavelet_name='haar', concat=1, **kwargs):\n        super(DWT, self).__init__(**kwargs)\n        # self._name = self.name + \"_\" + name\n        # get filter coeffs from 3rd party lib\n        wavelet = pywt.Wavelet(wavelet_name)\n        self.dec_len = wavelet.dec_len\n        self.concat = concat\n        # decomposition filter low pass and hight pass coeffs\n        db2_lpf = wavelet.dec_lo\n        db2_hpf = wavelet.dec_hi\n\n        # covert filters into tensors and reshape for convolution math\n        db2_lpf = tf.constant(db2_lpf[::-1])\n        self.db2_lpf = tf.reshape(db2_lpf, (1, wavelet.dec_len, 1, 1))\n\n        db2_hpf = tf.constant(db2_hpf[::-1])\n        self.db2_hpf = tf.reshape(db2_hpf, (1, wavelet.dec_len, 1, 1))\n\n        self.conv_type = \"VALID\"\n        self.border_padd = \"SYMMETRIC\"\n        self.wavelet_name = wavelet_name\n        self.concat = concat\n\n    def build(self, input_shape):\n        # filter dims should be bigger if input is not gray scale\n        if input_shape[-1] != 1:\n            # self.db2_lpf = tf.repeat(self.db2_lpf, input_shape[-1], axis=-1)\n            self.db2_lpf = tf.keras.backend.repeat_elements(self.db2_lpf, input_shape[-1], axis=-1)\n            # self.db2_hpf = tf.repeat(self.db2_hpf, input_shape[-1], axis=-1)\n            self.db2_hpf = tf.keras.backend.repeat_elements(self.db2_hpf, input_shape[-1], axis=-1)\n\n    def call(self, inputs, training=None, mask=None):\n\n        # border padding symatric add coulums\n        inputs_pad = tf.pad(inputs, [[0, 0], [0, 0], [self.dec_len-1, self.dec_len-1], [0, 0]], self.border_padd)\n\n        # approximation conv only rows\n        a = tf.nn.conv2d(\n            inputs_pad, self.db2_lpf, padding=self.conv_type, strides=[1, 1, 1, 1],\n        )\n        # details conv only rows\n        d = tf.nn.conv2d(\n            inputs_pad, self.db2_hpf, padding=self.conv_type, strides=[1, 1, 1, 1],\n        )\n        # ds - down sample\n        a_ds = a[:, :, 1:a.shape[2]:2, :]\n        d_ds = d[:, :, 1:d.shape[2]:2, :]\n\n        # border padding symatric add rows\n        a_ds_pad = tf.pad(a_ds, [[0, 0], [self.dec_len-1, self.dec_len-1], [0, 0], [0, 0]], self.border_padd)\n        d_ds_pad = tf.pad(d_ds, [[0, 0], [self.dec_len-1, self.dec_len-1], [0, 0], [0, 0]], self.border_padd)\n\n        # convolution is done on the rows so we need to\n        # transpose the matrix in order to convolve the colums\n        a_ds_pad = tf.transpose(a_ds_pad, perm=[0, 2, 1, 3])\n        d_ds_pad = tf.transpose(d_ds_pad, perm=[0, 2, 1, 3])\n\n        # aa approximation approximation\n        aa = tf.nn.conv2d(\n            a_ds_pad, self.db2_lpf, padding=self.conv_type, strides=[1, 1, 1, 1],\n        )\n        # ad approximation details\n        ad = tf.nn.conv2d(\n            a_ds_pad, self.db2_hpf, padding=self.conv_type, strides=[1, 1, 1, 1],\n        )\n        # ad details aproximation\n        da = tf.nn.conv2d(\n            d_ds_pad, self.db2_lpf, padding=self.conv_type, strides=[1, 1, 1, 1],\n        )\n        # dd details details\n        dd = tf.nn.conv2d(\n            d_ds_pad, self.db2_hpf, padding=self.conv_type, strides=[1, 1, 1, 1],\n        )\n\n        # transpose back the matrix\n        aa = tf.transpose(aa, perm=[0, 2, 1, 3])\n        ad = tf.transpose(ad, perm=[0, 2, 1, 3])\n        da = tf.transpose(da, perm=[0, 2, 1, 3])\n        dd = tf.transpose(dd, perm=[0, 2, 1, 3])\n\n        # down sample\n        ll = aa[:, 1:aa.shape[1]:2, :, :]\n        lh = ad[:, 1:ad.shape[1]:2, :, :]\n        hl = da[:, 1:da.shape[1]:2, :, :]\n        hh = dd[:, 1:dd.shape[1]:2, :, :]\n\n        # concate all outputs ionto tensor\n        if self.concat == 0:\n            x = tf.concat([ll, lh, hl, hh], axis=-1)\n        elif self.concat == 2:\n            x = ll\n        elif self.concat ==1:\n            return ll,lh,hl,hh\n        else:\n            x = tf.concat([tf.concat([ll, lh], axis=1), tf.concat([hl, hh], axis=1)], axis=2)\n        return x\n    def get_config(self):\n        config = super(DWT, self).get_config()\n        config.update({'wavelet_name': self.wavelet_name, 'concat': self.concat})\n        return config\n\n\n\ndef wavelet_conv_block(x, in_channels,name_prefix=''):\n    \n    ll, lh, hl, hh = DWT(concat=1)(x)\n    \n    y = tf.concat([ll, lh, hl, hh], axis=3)\n    \n    conv1 = tf.keras.layers.Conv2D(in_channels * 2, kernel_size=1, dilation_rate=1, padding='valid')(y)\n    \n    conv2 = tf.keras.layers.Conv2D(in_channels, kernel_size=3, dilation_rate=1, padding='same')(conv1)\n    conv2 = tf.keras.layers.BatchNormalization()(conv2,training=True)\n    conv2 = tf.keras.layers.ReLU()(conv2)\n\n    \n    conv3 = tf.keras.layers.Conv2D(in_channels, kernel_size=5, dilation_rate=1, padding='same')(conv1)\n    conv3 = tf.keras.layers.BatchNormalization()(conv3,training=True)\n    conv3 = tf.keras.layers.ReLU()(conv3)\n    \n    conv4 = tf.keras.layers.Conv2D(in_channels * 2, kernel_size=1, dilation_rate=1, padding='valid',name= name_prefix + 'out_wav')(tf.concat([conv2, conv3], axis=3))\n    \n    return conv4, ll, lh, hl, hh   ","metadata":{"execution":{"iopub.status.busy":"2024-09-14T18:23:06.246471Z","iopub.execute_input":"2024-09-14T18:23:06.246803Z","iopub.status.idle":"2024-09-14T18:23:06.344215Z","shell.execute_reply.started":"2024-09-14T18:23:06.246769Z","shell.execute_reply":"2024-09-14T18:23:06.343243Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Dual-mode Attention Gate (DAG)","metadata":{}},{"cell_type":"code","source":"def conv_block(x, filter_size, size, dropout, batch_norm=False):\n    \n    conv = layers.Conv2D(size, (filter_size, filter_size), padding=\"same\")(x)\n    if batch_norm is True:\n        conv = layers.BatchNormalization(axis=3)(conv,training=True)\n    conv = layers.ReLU()(conv)\n\n    conv = layers.Conv2D(size, (filter_size, filter_size), padding=\"same\")(conv)\n    if batch_norm is True:\n        conv = layers.BatchNormalization(axis=3)(conv,training=True)\n    conv = layers.ReLU()(conv)\n    \n    if dropout > 0:\n        conv = layers.Dropout(dropout)(conv)\n\n    return conv\n\n\ndef repeat_elem(tensor, rep):\n     return layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),arguments={'repnum': rep})(tensor)\n\n\ndef res_conv_block(x, filter_size, size, dropout, batch_norm=False):\n\n\n    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(x)\n    if batch_norm is True:\n        conv = layers.BatchNormalization(axis=3)(conv,training=True)\n    conv = layers.ReLU()(conv)\n    \n    conv = layers.Conv2D(size, (filter_size, filter_size), padding='same')(conv)\n    if batch_norm is True:\n        conv = layers.BatchNormalization(axis=3)(conv,training=True)\n    #conv = layers.Activation('relu')(conv)    #Activation before addition with shortcut\n    if dropout > 0:\n        conv = layers.Dropout(dropout)(conv)\n\n    shortcut = layers.Conv2D(size, kernel_size=(1, 1), padding='same')(x)\n    if batch_norm is True:\n        shortcut = layers.BatchNormalization(axis=3)(shortcut,training=True)\n\n    res_path = layers.add([shortcut, conv])\n    res_path = layers.ReLU()(res_path)    #Activation after addition with shortcut (Original residual block)\n    return res_path\n\ndef gating_signal(input, out_size, batch_norm=False):\n    init = RandomNormal(stddev=0.02)\n    x = layers.Conv2D(out_size, (1, 1), padding='same')(input)\n    if batch_norm:\n        x = layers.BatchNormalization(axis=3)(x,training=True)\n    x = layers.ReLU()(x)\n    return x\n\ndef attention_block(x, gating, inter_shape, name_prefix=''):\n    filters = x.shape[-1]\n    filtersg = gating.shape[-1]\n    shape_x = K.int_shape(x)\n    shape_g = K.int_shape(gating)\n    init = RandomNormal(stddev=0.02)\n    xa = x[:, :, :, :filters // 2]\n    xb = x[:, :, :, filters // 2: ]\n    gating_a = gating[:, :, :, :filtersg // 2]\n    gating_b = gating[:, :, :, filtersg // 2:]\n# Getting the x signal to the same shape as the gating signal\n    theta_xa = layers.Conv2D(inter_shape//2, (2, 2), strides=(2, 2), padding='same',name=name_prefix + 'theta_a')(xa)  # 16\n    shape_theta_xa = K.int_shape(theta_xa)\n    theta_xb = layers.Conv2D(inter_shape//2, (2, 2), strides=(2, 2), padding='same',name=name_prefix + 'theta_b')(xb)  # 16\n    shape_theta_xb = K.int_shape(theta_xb)\n# Getting the gating signal to the same number of filters as the inter_shape\n    phi_ga = layers.Conv2D(inter_shape//2, (1, 1), padding='same')(gating_a)\n    upsample_ga = layers.Conv2DTranspose(inter_shape//2, (3, 3),strides=(shape_theta_xa[1] // shape_g[1], shape_theta_xa[2] // shape_g[2]),padding='same',name=name_prefix + 'phi_ga')(phi_ga)  # 16\n    phi_gb = layers.Conv2D(inter_shape//2, (1, 1), padding='same')(gating_b)\n    upsample_gb = layers.Conv2DTranspose(inter_shape//2, (3, 3),strides=(shape_theta_xb[1] // shape_g[1], shape_theta_xb[2] // shape_g[2]),padding='same',name=name_prefix + 'phi_gb')(phi_gb)  # 16\n    \n    \n    ###################################################\n    concat_xg = layers.add([theta_xa,upsample_ga ],name=name_prefix + 'foreground_add')\n    act_xg = layers.ReLU()(concat_xg)\n    psi = layers.Conv2D(1, (1, 1), padding='same')(act_xg)\n    sigmoid_xg = layers.Activation('sigmoid')(psi)\n    shape_sigmoid = K.int_shape(sigmoid_xg)\n    upsample_psi1 = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]),name=name_prefix + 'visual_fore')(sigmoid_xg)  # 32\n\n    upsample_psi = repeat_elem(upsample_psi1, xa.shape[3])\n    ya = layers.multiply([upsample_psi, xa],name=name_prefix + 'foreground_out')\n\n    \n    ##################################################\n    subtract_xg = layers.subtract([theta_xb,upsample_gb],name=name_prefix + 'background_add')\n    sub_act_xg = layers.ReLU()(subtract_xg)\n    sub_psi = layers.Conv2D(1, (1, 1), padding='same')(sub_act_xg)\n    sub_sigmoid_xg = layers.Activation('sigmoid',name=name_prefix + 'before_reverse')(sub_psi)\n    sub_sigmoid_xg = -1 * (sub_sigmoid_xg) + 1\n    sub_upsample_psi1 = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]),name=name_prefix +'visual_back' )(sub_sigmoid_xg)  # 32\n    sub_upsample_psi = repeat_elem(sub_upsample_psi1, xb.shape[3])\n    yb = layers.multiply([sub_upsample_psi, xb],name=name_prefix + 'background_out')\n    ##################################################\n    y = layers.Concatenate(axis=3)([ya, yb])\n\n    result = layers.Conv2D(shape_x[3], (1, 1), padding='same')(y)\n    result_bn = layers.BatchNormalization(axis=3)(result,training=True)\n    result_bn = layers.ReLU(name=name_prefix + 'attention_out')(result_bn)\n    return result_bn,upsample_psi1,sub_upsample_psi1","metadata":{"execution":{"iopub.status.busy":"2024-09-14T18:23:06.348467Z","iopub.execute_input":"2024-09-14T18:23:06.348755Z","iopub.status.idle":"2024-09-14T18:23:06.383146Z","shell.execute_reply.started":"2024-09-14T18:23:06.348729Z","shell.execute_reply":"2024-09-14T18:23:06.382380Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Other Blocks","metadata":{}},{"cell_type":"code","source":"def conv2d_bn(x, filters, num_row, num_col, padding='same', strides=(1, 1),activation='relu', name=None):\n\n    init = RandomNormal(stddev=0.02)\n    x = Conv2D(filters, (num_row, num_col), strides=strides, padding=padding,kernel_initializer=init )(x)\n    x = BatchNormalization(axis=3)(x, training=True)\n\n    if(activation == None):\n        return x\n\n    x = Activation(activation, name=name)(x)\n\n    return x\n\ndef trans_conv2d_bn(x, filters, num_row, num_col, padding='same', strides=(2, 2), name=None,dropout=True):\n\n    x = Conv2DTranspose(filters, (num_row, num_col), strides=strides, padding=padding)(x)\n    x = BatchNormalization(axis=3)(x,training=True)\n    if dropout:\n        x = Dropout(0.5)(x, training=True)\n    \n    return x\n\n\ndef MultiResBlock(U, inp, alpha = 1.67):\n\n    W = alpha * U\n\n    shortcut = inp\n\n    shortcut = conv2d_bn(shortcut, int(W*0.167) + int(W*0.333) +\n                         int(W*0.5), 1, 1, activation=None, padding='same')\n\n    conv3x3 = conv2d_bn(inp, int(W*0.167), 3, 3,\n                        activation='relu', padding='same')\n\n    conv5x5 = conv2d_bn(conv3x3, int(W*0.333), 3, 3,\n                        activation='relu', padding='same')\n\n    conv7x7 = conv2d_bn(conv5x5, int(W*0.5), 3, 3,\n                        activation='relu', padding='same')\n\n    out = concatenate([conv3x3, conv5x5, conv7x7], axis=3)\n    out = BatchNormalization(axis=3)(out,training=True)\n\n    out = add([shortcut, out])\n    out = Activation('relu')(out)\n    out = BatchNormalization(axis=3)(out,training=True)\n\n    return out\n\ndef ResPath(filters, length, inp):\n\n    shortcut = inp\n    shortcut = conv2d_bn(shortcut, filters, 1, 1,activation=None, padding='same')\n\n    out = conv2d_bn(inp, filters, 3, 3, activation='relu', padding='same')\n\n    out = add([shortcut, out])\n    out = Activation('relu')(out)\n    out = BatchNormalization(axis=3)(out,training=True)\n\n    for i in range(length-1):\n\n        shortcut = out\n        shortcut = conv2d_bn(shortcut, filters, 1, 1,activation=None, padding='same')\n        out = conv2d_bn(out, filters, 3, 3, activation='relu', padding='same')\n        out = add([shortcut, out])\n        out = Activation('relu')(out)\n        out = BatchNormalization(axis=3)(out,training=True)\n\n    return out\n\ndef BasicConv2D(inputs,out_planes, kernel_size, stride=1, padding='same', dilation=1):\n    conv = tf.keras.layers.Conv2D(\n            filters=out_planes,\n            kernel_size=kernel_size,\n            strides=stride,\n            padding=padding,\n            dilation_rate=dilation,\n            use_bias=False\n        )(inputs)\n    bn = tf.keras.layers.BatchNormalization()(conv,training=True)\n    relu = tf.keras.layers.ReLU()(bn)\n    return relu\n    \n\ndef RFBModified(inputs, out_channel):\n    relu = tf.keras.layers.ReLU()(inputs)\n\n    # Define branch0\n    branch0 = BasicConv2D(relu, out_channel, kernel_size=1)\n\n    # Define branch1\n    conv1_1x1 = BasicConv2D(relu, out_channel, kernel_size=1)\n    conv1_1x3 = BasicConv2D(conv1_1x1, out_channel, kernel_size=(1, 3), padding='same')\n    conv1_3x1 = BasicConv2D(conv1_1x3, out_channel, kernel_size=(3, 1), padding='same')\n    conv1_3x3 = BasicConv2D(conv1_3x1, out_channel, kernel_size=3, padding='same', dilation=3)\n\n    # Define branch2\n    conv2_1x1 = BasicConv2D(relu, out_channel, kernel_size=1)\n    conv2_1x5 = BasicConv2D(conv2_1x1, out_channel, kernel_size=(1, 5), padding='same')\n    conv2_5x1 = BasicConv2D(conv2_1x5, out_channel, kernel_size=(5, 1), padding='same')\n    conv2_3x3 = BasicConv2D(conv2_5x1, out_channel, kernel_size=3, padding='same', dilation=5)\n\n    # Define branch3\n    conv3_1x1 = BasicConv2D(relu, out_channel, kernel_size=1)\n    conv3_1x7 = BasicConv2D(conv3_1x1, out_channel, kernel_size=(1, 7), padding='same')\n    conv3_7x1 = BasicConv2D(conv3_1x7, out_channel, kernel_size=(7, 1), padding='same')\n    conv3_3x3 = BasicConv2D(conv3_7x1, out_channel, kernel_size=3, padding='same', dilation=7)\n\n    # Concatenate branches\n    branches_concat = tf.keras.layers.Concatenate(axis=-1)([branch0, conv1_3x3, conv2_3x3, conv3_3x3])\n\n    # Final convolution and residual connection\n    conv_cat = BasicConv2D(branches_concat, out_channel, kernel_size=3, padding='same')\n    conv_res = BasicConv2D(relu, out_channel, kernel_size=1)\n\n    # Output\n    output = tf.keras.layers.ReLU()(conv_cat + conv_res)\n\n    return output\ndef aggregation(x1, x2, x3):\n    channel=32\n    upsample = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')\n\n    x1_1 = x1\n    x1_1_1 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x1_1)\n    x1_1_2 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x1_1_1)\n    x2_1_1 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x2)\n    x2_1 = tf.math.multiply(BasicConv2D(x1_1_1 ,channel, 3, padding='same'), x2)\n    x3_1 = BasicConv2D(x1_1_2,channel, 3, padding='same') \n    x3_1 = tf.math.multiply(x3_1 , BasicConv2D(x2_1_1,channel, 3, padding='same'))\n    x3_1 = tf.math.multiply(x3_1, x3)\n\n    x2_2 = tf.concat([x2_1, BasicConv2D(upsample(x1_1),2*channel, 3, padding='same')], axis=-1)\n    x2_2 = BasicConv2D(x2_2,2 * channel, 3, padding='same')\n\n    x3_2 = tf.concat([x3_1, BasicConv2D(upsample(x2_2),2 * channel, 3, padding='same')], axis=-1)\n    x3_2 = BasicConv2D(x3_2,3 * channel, 3, padding='same')\n\n    x = BasicConv2D(x3_2,3 * channel, 3, padding='same')\n    x = tf.keras.layers.Conv2D(1, 1)(x)\n    \n    return x\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T18:23:06.384570Z","iopub.execute_input":"2024-09-14T18:23:06.384860Z","iopub.status.idle":"2024-09-14T18:23:06.423074Z","shell.execute_reply.started":"2024-09-14T18:23:06.384832Z","shell.execute_reply":"2024-09-14T18:23:06.422157Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# GAN Architecture","metadata":{}},{"cell_type":"code","source":"def define_discriminator(image_shape):\n    \n    # weight initialization\n    init = RandomNormal(stddev=0.02) \n    # source image input\n    in_src_image = Input(shape=image_shape) \n    # target image input\n    in_target_image = Input(shape=(512,512,1)) \n    \n    # concatenate images, channel-wise\n    merged = Concatenate()([in_src_image, in_target_image])\n    \n    # C64: 4x4 kernel Stride 2x2\n    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n    d = LeakyReLU(alpha=0.2)(d)\n    # C128: 4x4 kernel Stride 2x2\n    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n    d = LeakyReLU(alpha=0.2)(d)\n    d = BatchNormalization()(d)\n    \n    # C256: 4x4 kernel Stride 2x2\n    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n    d = LeakyReLU(alpha=0.2)(d)\n    d = BatchNormalization()(d)\n    \n    # C512: 4x4 kernel Stride 2x2 \n    # Not in the original paper. Comment this block if you want.\n    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n    d = LeakyReLU(alpha=0.2)(d)\n    d = BatchNormalization()(d)\n    d = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n    patch_out = Activation('sigmoid')(d)\n    # define model\n    model = Model([in_src_image, in_target_image], patch_out)\n    # compile model\n    opt = Adam(lr=0.000009, beta_1=0.5)\n    model.compile(loss='mae', optimizer=opt, loss_weights=[0.5])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-09-14T18:23:06.424220Z","iopub.execute_input":"2024-09-14T18:23:06.424558Z","iopub.status.idle":"2024-09-14T18:23:06.438774Z","shell.execute_reply.started":"2024-09-14T18:23:06.424530Z","shell.execute_reply":"2024-09-14T18:23:06.437730Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\ndef define_generator(height, width, n_channels,name_prefix='f_'):\n    inputs = Input((height, width, n_channels))\n    encoder = VGG16(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n    encoder1 = DenseNet201(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n                           \n    mresblock1 = MultiResBlock(32, inputs)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(mresblock1)\n    mresblock1 = ResPath(32, 4, mresblock1)\n\n    \n    ######################################\n    wav_pool256 = DWT(name=\"haar\",concat=0)(inputs)\n    s2 = encoder.get_layer(\"block2_conv2\").output \n    e2 = encoder1.get_layer(\"conv1/relu\").output\n    mresblock2 = Concatenate()([pool1,s2,e2])\n    mresblock2 = MultiResBlock(64, mresblock2)\n    mresblock2 = Concatenate()([wav_pool256, mresblock2])\n    pool2 = MaxPooling2D(pool_size=(2, 2))(mresblock2)\n    mresblock2 = ResPath(64, 3, mresblock2)\n\n    ###########################################\n    mres_wav_pool256= DWT(concat=2)(mresblock1)\n    wav_cnn128 = Concatenate(name= name_prefix + 'wav_2_in')([wav_pool256, mresblock2,mres_wav_pool256])\n    wav_cnn128,_,_,_,_ = wavelet_conv_block(wav_cnn128, 16,name_prefix= name_prefix + 'wav_2')\n    \n    wav_out1_ll = DWT(concat=2)(inputs)\n    wav_pool128 = DWT(concat=0)(wav_out1_ll)\n    \n    s3 = encoder.get_layer(\"block3_conv2\").output\n    e3 = encoder1.get_layer(\"pool2_conv\").output\n    mresblock3 = Concatenate()([pool2,s3,e3])\n    mresblock3 = MultiResBlock(128, mresblock3)\n    mresblock3 = Concatenate()([wav_cnn128, mresblock3])\n    pool3 = MaxPooling2D(pool_size=(2, 2))(mresblock3)\n    mresblock3 = ResPath(128, 2, mresblock3)\n\n    #############################################\n    \n    \n    mres_wav_pool128 = DWT(concat=2)(mresblock2)\n    wav_cnn64 = Concatenate()([wav_pool128, mresblock3,mres_wav_pool128])\n    wav_cnn64,_,_,_,_ = wavelet_conv_block(wav_cnn64, 32,name_prefix= name_prefix + 'wav_3')    \n    \n    wav_out2_ll = DWT(concat=2)(wav_out1_ll)\n    wav_pool64 = DWT(concat=0)(wav_out2_ll)\n    \n    s4 = encoder.get_layer(\"block4_conv2\").output\n    e4 = encoder1.get_layer(\"pool3_conv\").output\n    mresblock4= Concatenate()([pool3,s4,e4])\n    mresblock4 = MultiResBlock(256, mresblock4)\n    mresblock4= Concatenate()([wav_cnn64, mresblock4])\n    pool4 = MaxPooling2D(pool_size=(2, 2))(mresblock4)\n    mresblock4 = ResPath(256, 1, mresblock4)\n    \n    \n    ###############################################\n    \n    mres_wav_pool64 = DWT(concat=2)(mresblock3)\n    wav_cnn32 = Concatenate()([wav_pool64, mresblock4,mres_wav_pool64])\n    wav_cnn32,_,_,_,_ = wavelet_conv_block(wav_cnn32, 64,name_prefix= name_prefix + 'wav_4')\n    \n    s5 = encoder.get_layer(\"block5_conv2\").output\n    e5 = encoder1.get_layer(\"pool4_conv\").output\n    mresblock5= Concatenate()([pool4,s5,e5])\n    mresblock5 = MultiResBlock(512, mresblock5)\n    mresblock5= Concatenate()([wav_cnn32, mresblock5])\n    pool5 = MaxPooling2D(pool_size=(2, 2))(mresblock5)\n    mresblock5 = ResPath(512, 1, mresblock5)\n    ##############################\n\n    mresblock6 = MultiResBlock(1024, pool5)\n\n    ###########################################\n    \n    gating_8 = gating_signal(mresblock6, 512, batch_norm=True)\n    att_8,_,_ = attention_block(mresblock5, gating_8, 512,name_prefix = name_prefix + 'at_1')\n    up5 = concatenate([trans_conv2d_bn(mresblock6, filters=512, num_row=2, num_col=2, padding='same', strides=(2, 2), dropout=True), att_8], axis=3)\n    mresblock7 = MultiResBlock(512, up5) \n    \n    \n    \n    \n    gating_16 = gating_signal(mresblock7, 256, batch_norm=True)\n    att_16,_,_ = attention_block(mresblock4, gating_16, 256,name_prefix = name_prefix + 'at_2')\n    up6 = concatenate([trans_conv2d_bn(mresblock7, filters=256, num_row=2, num_col=2, padding='same', strides=(2, 2), dropout=True), att_16], axis=3)\n    mresblock8 = MultiResBlock(256, up6)\n    \n    \n\n    gating_32 = gating_signal(mresblock8, 128, batch_norm=True)\n    att_32,at3_fore,at3_back = attention_block(mresblock3, gating_32, 128,name_prefix = name_prefix + 'at_3')\n    up7 = concatenate([trans_conv2d_bn(mresblock8, filters=128, num_row=2, num_col=2, padding='same', strides=(2, 2), dropout=True), att_32], axis=3)\n    mresblock9 = MultiResBlock(128, up7)\n\n\n    \n    \n    \n    gating_64 = gating_signal(mresblock9, 64, batch_norm=True)\n    att_64,at4_fore,at4_back = attention_block(mresblock2, gating_64, 64,name_prefix = name_prefix + 'at_4')\n    up8 = concatenate([trans_conv2d_bn(mresblock9, filters=64, num_row=2, num_col=2, padding='same', strides=(2, 2), dropout=False), att_64], axis=3)\n    mresblock10 = MultiResBlock(64, up8)\n\n    \n    \n\n    \n    \n    gating_128 = gating_signal(mresblock10, 32, batch_norm=True)\n    att_128,at5_fore,at5_back = attention_block(mresblock1, gating_128, 32,name_prefix = name_prefix + 'at_5')\n    up9 = concatenate([trans_conv2d_bn(mresblock10, filters=32, num_row=2, num_col=2, padding='same', strides=(2, 2), dropout=False), att_128], axis=3)\n    mresblock11 = MultiResBlock(32, up9)\n    \n    \n    init = RandomNormal(stddev=0.02)\n    conv10 = conv2d_bn(mresblock11, 1, 1, 1, activation='sigmoid')\n                           \n    \n    x3_rfb=RFBModified(mresblock3,32)\n    x4_rfb=RFBModified(mresblock4,32)\n    x5_rfb=RFBModified(mresblock5,32)\n\n    ra5_feat = aggregation(x5_rfb,x4_rfb,x3_rfb)\n\n\n    crop_5 = tf.image.resize(ra5_feat, [32,32])\n    x = -1 * (tf.math.sigmoid(crop_5)) + 1\n    x = tf.keras.layers.Multiply()([x, mresblock5])\n    \n    x = BasicConv2D(x, 256, 1)\n    x = BasicConv2D(x, 256, 5, padding='same')\n    x = BasicConv2D(x, 256, 5, padding='same')\n    x = BasicConv2D(x, 256, 5, padding='same')\n    ra4_feat = BasicConv2D(x, 1, 1)\n    x = tf.keras.layers.Add()([ra4_feat, crop_5])\n    crop_4 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)\n    x = -1 * (tf.math.sigmoid(crop_4)) + 1\n    x = tf.keras.layers.Multiply()([x, mresblock4])\n    x = BasicConv2D(x, 64, 1)\n    x = BasicConv2D(x, 64, 3, padding='same')\n    x = BasicConv2D(x, 64, 3, padding='same')\n    ra3_feat = BasicConv2D(x, 1, 3, padding='same')\n    x = tf.keras.layers.Add()([ra3_feat, crop_4])\n\n    crop_3 = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)\n    x = -1 * (tf.math.sigmoid(crop_3)) + 1\n    x = tf.keras.layers.Multiply()([x, mresblock3])\n    x = BasicConv2D(x, 64, 1)\n    x = BasicConv2D(x, 64, 3, padding='same')\n    x = BasicConv2D(x, 64, 3, padding='same')\n    ra2_feat = BasicConv2D(x, 1, 3, padding='same')\n    x = tf.keras.layers.Add()([ra2_feat, crop_3])\n    lateral_map_2 = tf.keras.layers.UpSampling2D(size=(4, 4), interpolation='bilinear')(x)\n    lateral_map_2 = tf.keras.activations.sigmoid(lateral_map_2)\n\n    out_image=conv10*lateral_map_2\n\n    model = Model(inputs, [out_image,at5_fore,at5_back,at4_fore,at4_back,at3_fore,at3_back])\n    \n\n    return model\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T18:23:06.440232Z","iopub.execute_input":"2024-09-14T18:23:06.440548Z","iopub.status.idle":"2024-09-14T18:23:06.485211Z","shell.execute_reply.started":"2024-09-14T18:23:06.440520Z","shell.execute_reply":"2024-09-14T18:23:06.484356Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def define_gan(g_model, d_model, image_shape):\n    # make weights in the discriminator not trainable\n    for layer in d_model.layers:\n        if not isinstance(layer, BatchNormalization):\n            layer.trainable = False       #Descriminator layers set to untrainable in the combined GAN but \n                                                #standalone descriminator will be trainable.\n            \n    # define the source image\n    in_src = Input(shape=image_shape)\n    # suppy the image as input to the generator \n    gen_out = g_model(in_src)\n    # supply the input image and generated image as inputs to the discriminator\n    dis_out = d_model([in_src, gen_out[0]])\n    # src image as input, generated image and disc. output as outputs\n    model = Model(in_src, [dis_out, gen_out[0], gen_out[1], gen_out[2], gen_out[3], gen_out[4], gen_out[5], gen_out[6]])\n    # compile model\n    opt = Adam(lr=0.00008, beta_1=0.5)\n    \n    model.compile(loss=['binary_crossentropy','binary_crossentropy','binary_crossentropy','binary_crossentropy','binary_crossentropy','binary_crossentropy','binary_crossentropy','binary_crossentropy'],optimizer=opt, loss_weights=[1,25,5,5,5,5,3,3])\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T18:23:06.486404Z","iopub.execute_input":"2024-09-14T18:23:06.486790Z","iopub.status.idle":"2024-09-14T18:23:06.499616Z","shell.execute_reply.started":"2024-09-14T18:23:06.486753Z","shell.execute_reply":"2024-09-14T18:23:06.498809Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Augmentation and data loader","metadata":{}},{"cell_type":"code","source":"import albumentations as A\naug1 = A.HorizontalFlip(p=1)\naug2 = A.VerticalFlip(p=1)\naug3 = A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=1)\naug4 = A.Blur(blur_limit=11, always_apply=True, p=1)\naug5 = A.Cutout(num_holes=8, max_h_size=96, max_w_size=96, fill_value=0, always_apply=True, p=1)\naug6 = A.Rotate(limit=90, interpolation=1, border_mode=2, value=None, mask_value=None, rotate_method='largest_box', crop_border=False, always_apply=True, p=1)\naug7 = A.Downscale(scale_min=0.25, scale_max=0.25, interpolation=None, always_apply=True, p=1)\naug8 = A.RandomBrightnessContrast (brightness_limit=0.4, contrast_limit=0.3, brightness_by_max=True, always_apply=True, p=1)\naug9 = A.HueSaturationValue(hue_shift_limit=0.3, sat_shift_limit=0.4, val_shift_limit=0.3, always_apply=True, p=1)\naug10 = A.Affine(scale=(0.2,0.3), translate_percent=0.2, rotate=(-30,30), shear=(-45,45), always_apply=True, p=1)\n\n# I used one augmentation type at a time, you can do multiple\ndef augment(image,mask,n):\n    if n==0:\n        augmented = aug10(image=image, mask=mask)\n    elif n==1:\n        augmented = aug1(image=image, mask=mask)\n    elif n==2: \n        augmented = aug2(image=image, mask=mask)\n    elif n==3: \n        augmented = aug3(image=image, mask=mask)\n    elif n==6:\n        augmented = aug6(image=image, mask=mask)\n    elif n==4: \n        augmented = aug4(image=image, mask=mask)\n    elif n==5: \n        augmented = aug5(image=image, mask=mask)\n    elif n==7:\n        return image,mask\n    elif n==8:\n        augmented = aug7(image=image, mask=mask)\n    elif n==9:\n        augmented = aug8(image=image, mask=mask)\n    elif n==10: \n        augmented = aug6(image=image, mask=mask)\n    else: \n        augmented = aug9(image=image, mask=mask)\n    \n        \n    image_aug= augmented['image']\n    mask_aug = augmented['mask']\n    return image_aug,mask_aug\n\ndef generate_real_samples(data_generator,mask_generator, n_samples,patch_shape, i):\n    ix = randint(0, len(data_generator), n_samples)\n    n = i % 12\n\n    # Initialize lists to store augmented images and masks\n    X1 = []\n    X2 = []\n    X3 = []\n    X4 = []\n    X5 = []\n    X6 = []\n    X7 = []\n\n\n    for i in ix:\n        augmented_image, augmented_mask = augment(\n            np.reshape(data_generator[i], [512,512, 3]),  # Input image\n            np.reshape(mask_generator[i], [512,512, 1]),  # Input mask\n            n  # Augmentation type\n        )\n        X1.append(augmented_image)\n        X2.append(augmented_mask)\n        X3.append(1-augmented_mask)\n        X4.append(tf.image.resize(augmented_mask,(256,256)))\n        X5.append(tf.image.resize(1-augmented_mask,(256,256)))\n        X6.append(tf.image.resize(augmented_mask,(128,128)))\n        X7.append(tf.image.resize(1-augmented_mask,(128,128)))\n    y = ones((n_samples, patch_shape, patch_shape, 1))\n    return [np.array(X1), np.array(X2), np.array(X3), np.array(X4), np.array(X5), np.array(X6), np.array(X7)],y\n\n\n# generate a batch of images, returns images and targets\ndef generate_fake_samples(g_model, samples, patch_shape):\n    # generate fake instance\n    X = g_model.predict(samples)\n    # create 'fake' class labels (0)\n    y = zeros((2, patch_shape, patch_shape, 1))\n    return X[0], y\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T18:23:06.500865Z","iopub.execute_input":"2024-09-14T18:23:06.501207Z","iopub.status.idle":"2024-09-14T18:23:07.851409Z","shell.execute_reply.started":"2024-09-14T18:23:06.501178Z","shell.execute_reply":"2024-09-14T18:23:07.850475Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/dropout/cutout.py:51: FutureWarning: Cutout has been deprecated. Please use CoarseDropout\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:1555: UserWarning: Using default interpolation INTER_NEAREST, which is sub-optimal.Please specify interpolation mode for downscale and upscale explicitly.For additional information see this PR https://github.com/albumentations-team/albumentations/pull/584\n  \"Using default interpolation INTER_NEAREST, which is sub-optimal.\"\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install natsort","metadata":{"execution":{"iopub.status.busy":"2024-09-14T18:23:07.854365Z","iopub.execute_input":"2024-09-14T18:23:07.854681Z","iopub.status.idle":"2024-09-14T18:23:19.502694Z","shell.execute_reply.started":"2024-09-14T18:23:07.854652Z","shell.execute_reply":"2024-09-14T18:23:19.501621Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Collecting natsort\n  Downloading natsort-8.4.0-py3-none-any.whl (38 kB)\nInstalling collected packages: natsort\nSuccessfully installed natsort-8.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport natsort\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ntrain1p = '/kaggle/input/busi-split/archive(3)/BUSI-splitted/train_folder/img'\ntrain2p = '/kaggle/input/busi-split/archive(3)/BUSI-splitted/train_folder/label'\nval1p = '/kaggle/input/busi-split/archive(3)/BUSI-splitted/val_folder/img'\nval2p = '/kaggle/input/busi-split/archive(3)/BUSI-splitted/val_folder/label'\ntest1p = '/kaggle/input/busi-split/archive(3)/BUSI-splitted/test_folder/img'\ntest2p = '/kaggle/input/busi-split/archive(3)/BUSI-splitted/test_folder/label'\ntrain1 = '/kaggle/input/busi-split/archive(3)/BUSI-splitted/train_folder/img/'\ntrain2 = '/kaggle/input/busi-split/archive(3)/BUSI-splitted/train_folder/label/'\nval1 = '/kaggle/input/busi-split/archive(3)/BUSI-splitted/val_folder/img/'\nval2 = '/kaggle/input/busi-split/archive(3)/BUSI-splitted/val_folder/label/'\ntest1 = '/kaggle/input/busi-split/archive(3)/BUSI-splitted/test_folder/img/'\ntest2 = '/kaggle/input/busi-split/archive(3)/BUSI-splitted/test_folder/label/'\n\nimport glob\nimport pandas as pd\n\ntrain1 = glob.glob(train1+ '/*.png')  # Adjust the file extension if needed\ntrain2 = glob.glob(train2 + '/*.png')\nval1 = glob.glob(val1 + '/*.png')\nval2 =glob.glob(val2 + '/*.png')\ntest1=glob.glob(test1 + '/*.png')\ntest2=glob.glob(test2 + '/*.png')\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain1 = natsort.natsorted(train1)\ntrain2 = natsort.natsorted(train2)\nval1 = natsort.natsorted(val1)\nval2 = natsort.natsorted(val2)\ntest1 = natsort.natsorted(test1)\ntest2 = natsort.natsorted(test2)\n# Split the data into train and test sets\ndf_image_train = pd.DataFrame(train1, columns=['path'])\ndf_mask_train = pd.DataFrame(train2, columns=['path'])\ndf_image_val = pd.DataFrame(val1, columns=['path'])\ndf_mask_val = pd.DataFrame(val2, columns=['path'])\ndf_image_test = pd.DataFrame(test1, columns=['path'])\ndf_mask_test = pd.DataFrame(test2, columns=['path'])\n\nimggen = ImageDataGenerator(rescale=1./255)\n\ntrain_data_generator = imggen.flow_from_dataframe(\n    dataframe=df_image_train,\n    x_col='path',\n    y_col=None,\n    directory=train1p,\n    target_size=(512,512),\n    color_mode='rgb',\n    class_mode=None,\n    shuffle=False,\n    batch_size=1,\n    interpolation='nearest'\n)\n\n\ntest_data_generator = imggen.flow_from_dataframe(\n    dataframe=df_image_test,\n    x_col='path',\n    y_col=None,\n    directory=test1p,\n    target_size=(512,512),\n    color_mode='rgb',\n    class_mode=None,\n    shuffle=False,\n    batch_size=1,\n    interpolation='nearest'\n)\n\nvalid_data_generator = imggen.flow_from_dataframe(\n    dataframe=df_image_val,\n    x_col='path',\n    y_col=None,\n    directory=val1p,\n    target_size=(512,512),\n    color_mode='rgb',\n    class_mode=None,\n    shuffle=False,\n    batch_size=1,\n    interpolation='nearest'\n)\n\n# Create ImageDataGenerator for masks\nmask_data_generator = imggen.flow_from_dataframe(\n    dataframe=df_mask_train,\n    x_col='path',\n    y_col=None,\n    directory=train2p,\n    target_size=(512,512),\n    color_mode='grayscale',\n    class_mode=None,\n    shuffle=False,\n    batch_size=1,\n    interpolation='nearest'\n)\n\n\nmask_data_generator_test = imggen.flow_from_dataframe(\n    dataframe=df_mask_test,\n    x_col='path',\n    y_col=None,\n    directory=test2p,\n    target_size=(512,512),\n    color_mode='grayscale',\n    class_mode=None,\n    shuffle=False,\n    batch_size=1,\n    interpolation='nearest'\n)\nmask_data_generator_valid = imggen.flow_from_dataframe(\n    dataframe=df_mask_val,\n    x_col='path',\n    y_col=None,\n    directory=val2p,\n    target_size=(512,512),\n    color_mode='grayscale',\n    class_mode=None,\n    shuffle=False,\n    batch_size=1,\n    interpolation='nearest'\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T18:23:19.504708Z","iopub.execute_input":"2024-09-14T18:23:19.505058Z","iopub.status.idle":"2024-09-14T18:23:19.973549Z","shell.execute_reply.started":"2024-09-14T18:23:19.505022Z","shell.execute_reply":"2024-09-14T18:23:19.972580Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Found 517 validated image filenames.\nFound 65 validated image filenames.\nFound 65 validated image filenames.\nFound 517 validated image filenames.\nFound 65 validated image filenames.\nFound 65 validated image filenames.\n","output_type":"stream"}]},{"cell_type":"code","source":"def summarize_performance(step, g_model1,g_model2):\n\n    filename2 = 'lesion_model_%06d.h5' % (step+1)\n    g_model1.save(filename2)\n    filename4 = 'background_model_%06d.h5' % (step+1)\n    g_model2.save(filename4)\n    print('>Saved: %s and %s' % (filename2,filename4))\n\ndef remove_prev_performance(score_epoch):\n    filename3 = 'lesion_model_%06d.h5' % (score_epoch)\n    filename5 = 'background_model_%06d.h5' % (score_epoch)\n    os.remove(\"/kaggle/working/\"+filename3)\n    os.remove(\"/kaggle/working/\"+filename5)\n    print('>Removed: %s and %s and updated' % (filename3,filename5))\n\ndef summarize_performancet(step, g_model1,g_model2,d_model1,d_model2):\n\n    filename2 = 'lesion_model_%06d.h5' % (step+1)\n    g_model1.save(filename2)\n    filename4 = 'background_model_%06d.h5' % (step+1)\n    g_model2.save(filename4)\n    filename1 = 'lesion_dis_%06d.h5' % (step+1)\n    d_model1.save(filename1)\n    filename3 = 'background_dis_%06d.h5' % (step+1)\n    d_model2.save(filename3)\n    print('>Saved: %s and %s , %s and %s' % (filename2,filename4,filename1,filename3))\n\ndef remove_prev_performancet(score_epoch):\n    filename3 = 'lesion_model_%06d.h5' % (score_epoch)\n    filename5 = 'background_model_%06d.h5' % (score_epoch)\n    filename7 = 'lesion_dis_%06d.h5' % (score_epoch)\n    filename9 = 'background_dis_%06d.h5' % (score_epoch)\n    os.remove(\"/kaggle/working/\"+filename3)\n    os.remove(\"/kaggle/working/\"+filename5)\n    os.remove(\"/kaggle/working/\"+filename7)\n    os.remove(\"/kaggle/working/\"+filename9)\n    print('>Removed: %s and %s, %s and %s and updated' % (filename3,filename5,filename7,filename9))    \n    \n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T18:23:19.974726Z","iopub.execute_input":"2024-09-14T18:23:19.975022Z","iopub.status.idle":"2024-09-14T18:23:19.987015Z","shell.execute_reply.started":"2024-09-14T18:23:19.974992Z","shell.execute_reply":"2024-09-14T18:23:19.986139Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Additional Losses","metadata":{}},{"cell_type":"code","source":"from tensorflow.image import ssim\n\ndef log_ssim_mse_loss(y_true, y_pred, alpha=0.4):\n    ssim = tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n    loss = -tf.math.log(ssim) * alpha + mse * (1 - alpha)\n    return loss*5\n\ndef boundary_loss(y_true, y_pred):\n    # Compute the gradient of the predicted and ground truth masks\n    dy_true, dx_true = tf.image.image_gradients(y_true)\n    dy_pred, dx_pred = tf.image.image_gradients(y_pred)\n\n    # Compute the boundary term of the loss function\n    term_1 = tf.abs(tf.reduce_mean(tf.abs(dy_true) - tf.abs(dy_pred)))\n    term_2 = tf.abs(tf.reduce_mean(tf.abs(dx_true) - tf.abs(dx_pred)))\n\n    # Return the sum of the two terms as the boundary loss\n    return (term_1 + term_2)*5\n\n\ndef mae_loss(y_true, y_pred):\n    return mean_absolute_error(tf.reshape(y_true, [-1]), tf.reshape(y_pred, [-1]))","metadata":{"execution":{"iopub.status.busy":"2024-09-14T18:23:19.988247Z","iopub.execute_input":"2024-09-14T18:23:19.988677Z","iopub.status.idle":"2024-09-14T18:23:20.002917Z","shell.execute_reply.started":"2024-09-14T18:23:19.988635Z","shell.execute_reply":"2024-09-14T18:23:20.002132Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"\ndef train(d_model, g_model, gan_model,d_model2,g_model2,gan_model2, train_data_generator,mask_data_generator, n_epochs=100, n_batch=1):\n    n_patch = d_model.output_shape[1]\n    bat_per_epo = int(len(train_data_generator)/ n_batch)\n    # calculate the number of training iterations\n    n_steps = bat_per_epo * n_epochs\n    print('starting')\n    dloss11=[]\n    dloss21=[]\n    gloss1=[]\n    score1=[]\n    Bdloss11=[]\n    Bdloss21=[]\n    rloss11=[]\n    Bgloss1=[]\n    Bscore1=[]\n    merge_score=[]\n    epochss=[]\n    epochss = list(range(0,n_epochs))\n    score_max=0\n    score_epoch=0\n    # manually enumerate epochs\n    for j in range(n_epochs):\n        dloss1=[]\n        dloss2=[]\n        gloss=[]\n        Bdloss1=[]\n        Bdloss2=[]\n        Bgloss=[]\n        rloss=[]\n        iou_score=[]\n        iou_scoreB=[]\n        iou_score_merge=[]\n        maxv1=0\n        count1=0\n        maxv2=0\n        count2=0\n        for i in range(bat_per_epo):\n        ##for lesion\n        # select a batch of real samples\n            [X_realA, X_realB, X_realC,X_realB256,X_realC256,X_realB128,X_realC128], y_real = generate_real_samples(train_data_generator,mask_data_generator, n_batch, n_patch,i)\n        # generate a batch of fake samples\n            X_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n            X_fakeC, y_fake = generate_fake_samples(g_model2, X_realA, n_patch)\n        # update discriminator for real samples\n            d_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n        # update discriminator for generated samples\n            d_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n            \n            Bd_loss1 = d_model2.train_on_batch([X_realA, X_realC], y_real)\n        # update discriminator for generated samples\n            Bd_loss2 = d_model2.train_on_batch([X_realA, X_fakeC], y_fake)\n        # update the generator\n        \n            X_realA_s = (X_realA)\n            X_fakeB1 = (X_fakeB)\n            X_fakeB1 = np.around(X_fakeB1)\n            BX_fakeB1 = (X_fakeC)\n            BX_fakeB1 = np.around(BX_fakeB1)\n            X_fakeBnew = tf.cast(np.logical_or(X_fakeB1,BX_fakeB1), tf.float32) \n            X_realA_recons = tf.cast(np.multiply(X_realA_s,X_fakeBnew),tf.float32)\n            BX_fakeB2 = (1-X_fakeC)\n            BX_fakeB2 = (BX_fakeB2)\n            BX_fakeB2 = np.around(BX_fakeB2)\n            X_fakeBnew2 = tf.cast(np.logical_and(X_fakeB1,BX_fakeB2), tf.float32) \n            reconstruction_loss = log_ssim_mse_loss(X_realA_s, X_realA_recons) + (tf.keras.backend.mean(tf.keras.losses.MAE( tf.cast(X_realB, tf.float32), X_fakeBnew2))*2) + boundary_loss(tf.cast(X_realB, tf.float32), X_fakeBnew2)\n            gan_model.add_loss(lambda:reconstruction_loss)\n            gan_model2.add_loss(lambda:reconstruction_loss)  \n            \n            \n            g_loss, _, _, _, _, _, _ , _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB,X_realB,X_realB,X_realB256,X_realC256,X_realB128,X_realB128])\n            Bg_loss, _, _, _, _, _, _, _, _ = gan_model2.train_on_batch(X_realA, [y_real, X_realC,X_realC,X_realC,X_realB256,X_realC256,X_realC128,X_realC128])\n\n\n            dloss1.append(d_loss1)\n            dloss2.append(d_loss2)\n            gloss.append(g_loss)\n            rloss.append(reconstruction_loss)\n            Bdloss1.append(Bd_loss1)\n            Bdloss2.append(Bd_loss2)\n            Bgloss.append(Bg_loss)\n\n        test_iou_score1and=[]\n        test_iou_score1or=[]\n        testf=[]\n        testb=[]\n        l=0\n        for l in range(len(test_data_generator)):\n            output=tf.reshape(test_data_generator[l], [1, 512,512, 3])\n            gen_lesion_image1 = g_model.predict(output)\n            gen_back_image1 = g_model2.predict(output)\n            gen_lesion_image1 = (gen_lesion_image1[0])\n            gen_lesion_image1 = np.around(gen_lesion_image1)\n            gen_back_image1 = (1-gen_back_image1[0])\n            gen_back_image1 = np.around(gen_back_image1)\n            \n            tar_image1 = np.around(mask_data_generator_test[l])\n            gen_image1 = numpy.logical_and(gen_lesion_image1,gen_back_image1 )\n            gen_image1or = numpy.logical_or(gen_lesion_image1,gen_back_image1 )\n            intersection1 = numpy.logical_and(gen_image1, tar_image1)\n            union1 = numpy.logical_or(gen_image1, tar_image1)\n            test_iou_score1and.append(numpy.sum(intersection1) / numpy.sum(union1))\n            intersection1or = numpy.logical_and(gen_image1or, tar_image1)\n            union1or = numpy.logical_or(gen_image1or, tar_image1)\n            test_iou_score1or.append(numpy.sum(intersection1or) / numpy.sum(union1or))\n        test_iou_score2and=numpy.mean(test_iou_score1and)\n        test_iou_score2or=numpy.mean(test_iou_score1or)\n        \n        valid_iou_score1and=[]\n        valid_iou_score1or=[]\n        l=0\n        for l in range(len(valid_data_generator)):\n            output=tf.reshape(valid_data_generator[l], [1, 512,512, 3])\n            gen_lesion_image1 = g_model.predict(output)\n            gen_back_image1 = g_model2.predict(output)\n            gen_lesion_image1 = (gen_lesion_image1[0])\n            gen_lesion_image1 = np.around(gen_lesion_image1)\n            gen_back_image1 = (1-gen_back_image1[0])\n            gen_back_image1 = np.around(gen_back_image1)\n            \n            tar_image1 = np.around(mask_data_generator_valid[l])\n            gen_image1 = numpy.logical_and(gen_lesion_image1,gen_back_image1 )\n            gen_image1or = numpy.logical_or(gen_lesion_image1,gen_back_image1)\n            intersection1 = numpy.logical_and(gen_image1, tar_image1)\n            union1 = numpy.logical_or(gen_image1, tar_image1)\n            valid_iou_score1and.append(numpy.sum(intersection1) / numpy.sum(union1))\n            intersection1or = numpy.logical_and(gen_image1or, tar_image1)\n            union1or = numpy.logical_or(gen_image1or, tar_image1)\n            valid_iou_score1or.append(numpy.sum(intersection1or) / numpy.sum(union1or))\n        valid_iou_score2and=numpy.mean(valid_iou_score1and)\n        valid_iou_score2or=numpy.mean(valid_iou_score1or)\n        \n        lr_scheduler_f.on_epoch_end(j+1, {'val_iou': max(valid_iou_score2and,valid_iou_score2or)})\n        lr_scheduler_b.on_epoch_end(j+1, {'val_iou': max(valid_iou_score2and,valid_iou_score2or)})\n\n        dloss1m=numpy.mean(dloss1)\n        dloss2m=numpy.mean(dloss2)\n        glossm=numpy.mean(gloss)\n        Bdloss1m=numpy.mean(Bdloss1)\n        Bdloss2m=numpy.mean(Bdloss2)\n        rlossm=numpy.mean(rloss)\n        Bglossm=numpy.mean(Bgloss)\n        print('Epoch %d> d1[%.3f] d2[%.3f]  g[%.3f] Bd1[%.3f] Bd2[%.3f]  Bg[%.3f]  r[%.3f] valid and:[%.3f] valid or:[%.3f]  test_iou_and:[%.5f] test_iou_OR:[%.5f]' % (j+1, dloss1m, dloss2m, glossm,Bdloss1m, Bdloss2m,  Bglossm,rlossm, valid_iou_score2and,valid_iou_score2or,test_iou_score2and,test_iou_score2or))\n        dloss11.append(dloss1m)\n        dloss21.append(dloss2m)\n        gloss1.append(glossm)\n        Bdloss11.append(Bdloss1m)\n        Bdloss21.append(Bdloss2m)\n        rloss11.append(rlossm)\n        Bgloss1.append(Bglossm)\n        # summarize performance\n        # best validation score based decision\n        if max(valid_iou_score2and,valid_iou_score2or)>=score_max:\n            summarize_performance((j), g_model,g_model2)\n            if j!=0:\n                remove_prev_performance(score_epoch)\n            score_max=max(valid_iou_score2and,valid_iou_score2or)\n            score_epoch=j+1\n        # best test score based decision\n        if max(test_iou_score2and,test_iou_score2or)>=score_max:\n            summarize_performance((j), g_model,g_model2)\n            if j!=0:\n                remove_prev_performance(score_epoch)\n            score_max=max(test_iou_score2and,test_iou_score2or)\n            score_epoch=j+1\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T19:06:18.267275Z","iopub.execute_input":"2024-09-14T19:06:18.267687Z","iopub.status.idle":"2024-09-14T19:06:18.310470Z","shell.execute_reply.started":"2024-09-14T19:06:18.267650Z","shell.execute_reply":"2024-09-14T19:06:18.309544Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Calling everything to run","metadata":{}},{"cell_type":"code","source":"image_shape = (512,512,3)\n# define the models\nd_model_fore = define_discriminator(image_shape)\ng_model_fore = define_generator(height=512, width=512, n_channels=3,name_prefix='f_')\nd_model_back = define_discriminator(image_shape)\ng_model_back = define_generator(height=512, width=512, n_channels=3,name_prefix='b_')\ngan_model_fore = define_gan(g_model_fore, d_model_fore, image_shape)\ngan_model_back = define_gan(g_model_back, d_model_back, image_shape)\n\nfrom tensorflow.keras.callbacks import Callback \nclass LearningRateSchedulerWithPatienceIoU(Callback):\n    def __init__(self, reduce_lr_factor, patience, min_lr, monitor='val_iou',model= None):\n        super(LearningRateSchedulerWithPatienceIoU, self).__init__()\n        self.reduce_lr_factor = reduce_lr_factor\n        self.patience = patience\n        self.min_lr = min_lr\n        self.monitor = monitor\n        self.wait = 0\n        self.best_iou = -float('inf')\n        self.model = model\n    def on_epoch_end(self, epoch, logs=None):\n        current_iou = logs.get(self.monitor, -float('inf'))\n\n        if current_iou > self.best_iou:\n            self.best_iou = current_iou\n            self.wait = 0\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                new_lr = self.model.optimizer.lr.numpy() * self.reduce_lr_factor\n                new_lr = max(new_lr, self.min_lr)\n                self.model.optimizer.lr.assign(new_lr)\n                print(f\"\\nReduced learning rate to {new_lr} after {self.patience} epochs without improvement in IoU.\\n\")\n                self.wait = 0\n\n\ninitial_learning_rate = 0.00008  # Initial learning rate\nreduce_lr_factor = 0.5  # Factor by which to reduce the learning rate\npatience = 30  # Number of epochs without improvement in IoU before reducing learning rate\nmin_learning_rate = 1e-8  # Minimum learning rate\n\n# Create the custom learning rate scheduler\nlr_scheduler_f = LearningRateSchedulerWithPatienceIoU(reduce_lr_factor, patience, min_learning_rate,model=gan_model_fore)\nlr_scheduler_b = LearningRateSchedulerWithPatienceIoU(reduce_lr_factor, patience, min_learning_rate,model=gan_model_back)\n\n\nfrom datetime import datetime \nstart1 = datetime.now() \n\ntrain(d_model_fore, g_model_fore, gan_model_fore,d_model_back,g_model_back,gan_model_back,train_data_generator,mask_data_generator, n_epochs=300, n_batch=2) \n\n\nstop1 = datetime.now()\n#Execution time of the model \nexecution_time = stop1-start1\nprint(\"Execution time is: \", execution_time)","metadata":{"execution":{"iopub.status.busy":"2024-09-14T19:06:26.828695Z","iopub.execute_input":"2024-09-14T19:06:26.829744Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"starting\nEpoch 1> d1[0.138] d2[0.243]  g[17.759] Bd1[0.119] Bd2[0.206]  Bg[28.532]  r[3.375] valid and:[0.395] valid or:[0.212]  test_iou_and:[0.34787] test_iou_OR:[0.20593]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n  category=CustomMaskWarning)\n","output_type":"stream"},{"name":"stdout","text":">Saved: lesion_model_000001.h5 and background_model_000001.h5\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}